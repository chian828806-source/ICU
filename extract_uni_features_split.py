# File: extract_uni_features_split.py

import os
import numpy as np
from PIL import Image
import torch
import timm
from torchvision import transforms
from tqdm import tqdm

# -------------------------------
# 1. é…ç½®å‚æ•°
# -------------------------------
# UNI æ¨¡å‹ç›¸å…³è·¯å¾„
UNI_MODEL_DIR = r"E:\py_ai\python-code\ICT_related\UNI"  # åŒ…å« pytorch_model.bin çš„ç›®å½•
# è®­ç»ƒé›†å›¾åƒç›®å½• (å¢å¼ºåçš„)
TRAIN_IMAGE_DIR = r"E:\py_ai\python-code\ICT_related\UNI_Photos_Split_Enhanced"
# éªŒè¯é›†å›¾åƒç›®å½• (åŸå§‹çš„)
VAL_IMAGE_DIR = r"E:\py_ai\python-code\ICT_related\UNI_Photos_Split\val"
# ç‰¹å¾å’Œæ ‡ç­¾ä¿å­˜ç›®å½•
FEATURES_OUTPUT_DIR = r"E:\py_ai\python-code\ICT_related\Data_train"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"


# -------------------------------
# 2. åŠ è½½æœ¬åœ° UNI æ¨¡å‹å’Œé¢„å¤„ç†å™¨
# -------------------------------
def load_local_uni_model():
    """åŠ è½½æœ¬åœ°çš„ UNI æ¨¡å‹æƒé‡ (åŸºäº ViT-L/16)"""
    print("æ­£åœ¨åŠ è½½æœ¬åœ° UNI æ¨¡å‹...")

    # 1. å®šä¹‰æ¨¡å‹æ¶æ„ (ViT-L/16) - ç§»é™¤ dynamic_img_size å‚æ•°
    model = timm.create_model(
        "vit_large_patch16_224",
        img_size=224,
        patch_size=16,
        init_values=1e-5,
        num_classes=0,  # ç§»é™¤åˆ†ç±»å¤´ï¼Œåªæå–ç‰¹å¾
        # dynamic_img_size=True # ç§»é™¤æ­¤å‚æ•°ï¼Œé¿å…é”™è¯¯
    )

    # 2. åŠ è½½æœ¬åœ°æƒé‡
    checkpoint_path = os.path.join(UNI_MODEL_DIR, "pytorch_model.bin")
    if not os.path.exists(checkpoint_path):
        raise FileNotFoundError(f"UNI æ¨¡å‹æƒé‡æ–‡ä»¶æœªæ‰¾åˆ°: {checkpoint_path}")

    state_dict = torch.load(checkpoint_path, map_location="cpu")
    model.load_state_dict(state_dict, strict=True)

    # 3. è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    model.eval()
    model.to(DEVICE)

    # 4. å®šä¹‰é¢„å¤„ç†å™¨ (æ ¹æ®å®˜æ–¹æ–‡æ¡£ï¼Œä½¿ç”¨ ImageNet å½’ä¸€åŒ–å‚æ•°)
    transform = transforms.Compose([
        transforms.Resize(224),
        transforms.ToTensor(),
        transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
    ])

    print("âœ… æœ¬åœ° UNI æ¨¡å‹åŠ è½½å®Œæˆ")
    return model, transform


# -------------------------------
# 3. æå–å›¾åƒç‰¹å¾
# -------------------------------
def extract_features(image_dir, model, transform, description=""):
    """ä»æŒ‡å®šç›®å½•ä¸‹çš„æ‰€æœ‰å›¾åƒæå–ç‰¹å¾"""
    print(f"å¼€å§‹æå– {description} ç‰¹å¾...")

    all_features = []
    all_labels = []

    class_names = ['Normal', 'Benign', 'InSitu', 'Invasive']
    label_map = {name: idx for idx, name in enumerate(class_names)}

    for class_name in class_names:
        class_path = os.path.join(image_dir, class_name)
        if not os.path.exists(class_path):
            print(f"âš ï¸  è·³è¿‡ä¸å­˜åœ¨çš„ç±»åˆ«ç›®å½•: {class_path}")
            continue

        img_files = [f for f in os.listdir(class_path) if f.lower().endswith(('.tif', '.png', '.jpg', '.jpeg'))]
        print(f"å¤„ç†ç±»åˆ« {class_name} ({description}): {len(img_files)} å¼ å›¾åƒ")

        for img_name in tqdm(img_files, desc=f"æå– {class_name} ({description}) ç‰¹å¾"):
            img_path = os.path.join(class_path, img_name)

            try:
                # åŠ è½½å›¾åƒ
                image = Image.open(img_path).convert('RGB')
                # é¢„å¤„ç†
                image_tensor = transform(image).unsqueeze(0).to(DEVICE)  # [1, 3, 224, 224]

                # æå–ç‰¹å¾
                with torch.no_grad():
                    features = model(image_tensor)  # [1, 1024]

                all_features.append(features.squeeze().cpu().numpy())  # [1024,]
                all_labels.append(label_map[class_name])

            except Exception as e:
                print(f"âš ï¸  å¤„ç† {img_name} æ—¶å‡ºé”™: {e}")
                continue

    features_array = np.array(all_features)  # [N, 1024]
    labels_array = np.array(all_labels)  # [N,]

    print(f"âœ… {description} ç‰¹å¾æå–å®Œæˆ: {features_array.shape}, {labels_array.shape}")
    return features_array, labels_array


# -------------------------------
# 4. ä¿å­˜ç‰¹å¾å’Œæ ‡ç­¾
# -------------------------------
def save_features_and_labels(features, labels, features_path, labels_path):
    """ä¿å­˜ç‰¹å¾å’Œæ ‡ç­¾æ•°ç»„"""
    os.makedirs(os.path.dirname(features_path), exist_ok=True)

    np.save(features_path, features)
    np.save(labels_path, labels)

    print(f"âœ… ç‰¹å¾å·²ä¿å­˜è‡³: {features_path}")
    print(f"âœ… æ ‡ç­¾å·²ä¿å­˜è‡³: {labels_path}")


# -------------------------------
# 5. ä¸»å‡½æ•°
# -------------------------------
def main():
    # 1. åŠ è½½æ¨¡å‹
    model, transform = load_local_uni_model()

    # 2. æå–å¢å¼ºè®­ç»ƒé›†ç‰¹å¾
    train_features, train_labels = extract_features(
        TRAIN_IMAGE_DIR, model, transform, description="å¢å¼ºè®­ç»ƒé›†"
    )

    # 3. æå–åŸå§‹éªŒè¯é›†ç‰¹å¾
    val_features, val_labels = extract_features(
        VAL_IMAGE_DIR, model, transform, description="åŸå§‹éªŒè¯é›†"
    )

    # 4. ä¿å­˜ç‰¹å¾å’Œæ ‡ç­¾
    # è®­ç»ƒé›†
    save_features_and_labels(
        train_features, train_labels,
        os.path.join(FEATURES_OUTPUT_DIR, "X_train_enhanced.npy"),
        os.path.join(FEATURES_OUTPUT_DIR, "y_train_enhanced.npy")
    )
    # éªŒè¯é›†
    save_features_and_labels(
        val_features, val_labels,
        os.path.join(FEATURES_OUTPUT_DIR, "X_val_total.npy"),
        os.path.join(FEATURES_OUTPUT_DIR, "y_val_total.npy")
    )

    print("\nğŸ‰ UNI ç‰¹å¾æå–ä¸ä¿å­˜å®Œæˆï¼")
    print(f"å¢å¼ºè®­ç»ƒé›†ç‰¹å¾å½¢çŠ¶: {train_features.shape}")
    print(f"åŸå§‹éªŒè¯é›†ç‰¹å¾å½¢çŠ¶: {val_features.shape}")
    print(f"å¢å¼ºè®­ç»ƒé›†æ ‡ç­¾å½¢çŠ¶: {train_labels.shape}")
    print(f"åŸå§‹éªŒè¯é›†æ ‡ç­¾å½¢çŠ¶: {val_labels.shape}")
    print(f"æ ‡ç­¾åˆ†å¸ƒ (è®­ç»ƒé›†): {dict(zip(*np.unique(train_labels, return_counts=True)))}")
    print(f"æ ‡ç­¾åˆ†å¸ƒ (éªŒè¯é›†): {dict(zip(*np.unique(val_labels, return_counts=True)))}")


# -------------------------------
# 6. ç¨‹åºå…¥å£
# -------------------------------
if __name__ == "__main__":
    main()