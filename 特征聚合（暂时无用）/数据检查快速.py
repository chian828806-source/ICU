import numpy as np
import matplotlib
# è®¾ç½®matplotlibåç«¯ä¸ºTkAggï¼ˆé€‚åˆWindowsç¯å¢ƒï¼‰
matplotlib.use('TkAgg')
# æ·»åŠ ä¸­æ–‡å­—ä½“è®¾ç½®
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
import pandas as pd
plt.rcParams['font.sans-serif'] = ['SimHei']  # ç”¨æ¥æ­£å¸¸æ˜¾ç¤ºä¸­æ–‡æ ‡ç­¾
plt.rcParams['axes.unicode_minus'] = False    # ç”¨æ¥æ­£å¸¸æ˜¾ç¤ºè´Ÿå·
# ç§»é™¤äº¤äº’æ¨¡å¼è®¾ç½®ï¼Œä½¿ç”¨éäº¤äº’æ¨¡å¼

# å®šä¹‰ä¸€ä¸ªç¡®ä¿å›¾è¡¨æ˜¾ç¤ºçš„å‡½æ•°
def ensure_plot_shows():
    """ç¡®ä¿å›¾è¡¨æ˜¾ç¤º"""
    plt.show(block=True)  # ä½¿ç”¨block=Trueç¡®ä¿å›¾è¡¨çª—å£ä¸ä¼šç«‹å³å…³é—­

def correct_data_analysis(features_path, labels_path):
    """ä¿®æ­£çš„æ•°æ®åˆ†æ - é’ˆå¯¹å·²èšåˆçš„ç‰¹å¾"""
    
    print("ğŸš€ ä¿®æ­£ç‰ˆæ•°æ®æ£€æŸ¥å¼€å§‹...")
    
    # åŠ è½½æ•°æ®
    features = np.load(features_path)  # å½¢çŠ¶: (400, 1024)
    labels = np.load(labels_path)      # å½¢çŠ¶: (400,)
    
    print(f"ğŸ“Š ç‰¹å¾å½¢çŠ¶: {features.shape}")
    print(f"ğŸ·ï¸  æ ‡ç­¾å½¢çŠ¶: {labels.shape}")
    
    # åŸºæœ¬éªŒè¯
    if len(features) != len(labels):
        print("âŒ é”™è¯¯: ç‰¹å¾å’Œæ ‡ç­¾æ•°é‡ä¸åŒ¹é…!")
        return None, None
    
    # æ•°æ®ç»Ÿè®¡
    n_wsis, feature_dim = features.shape
    print(f"âœ… æ•°æ®æ ¼å¼: å·²èšåˆçš„ç‰¹å¾")
    print(f"ğŸ“ˆ WSIæ•°é‡: {n_wsis}")
    print(f"ğŸ¯ ç‰¹å¾ç»´åº¦: {feature_dim}")
    
    # æ ‡ç­¾åˆ†å¸ƒ
    unique_labels, counts = np.unique(labels, return_counts=True)
    print(f"ğŸ“‹ æ ‡ç­¾åˆ†å¸ƒ:")
    for label, count in zip(unique_labels, counts):
        print(f"  ç±»åˆ« {label}: {count} ä¸ªæ ·æœ¬ ({count/len(labels)*100:.1f}%)")
    
    # ç‰¹å¾è´¨é‡æ£€æŸ¥
    print(f"ğŸ” ç‰¹å¾å€¼èŒƒå›´: [{features.min():.3f}, {features.max():.3f}]")
    print(f"ğŸ“ ç‰¹å¾å‡å€¼: {features.mean():.3f} Â± {features.std():.3f}")
    
    # æ£€æŸ¥NaNå’Œæ— é™å€¼
    nan_count = np.sum(np.isnan(features))
    inf_count = np.sum(np.isinf(features))
    print(f"ğŸ§¹ æ•°æ®æ¸…æ´åº¦ - NaN: {nan_count}, æ— é™å€¼: {inf_count}")
    
    return features, labels

def visualize_aggregated_features(features, labels):
    """å¯è§†åŒ–å·²èšåˆçš„ç‰¹å¾"""
    
    print("\nğŸ“Š å¼€å§‹ç‰¹å¾å¯è§†åŒ–...")
    
    # åˆ›å»ºæ–°çš„å›¾å½¢ï¼Œç¡®ä¿ä¸ä¼šä¸å…¶ä»–å›¾å½¢å†²çª
    plt.figure(figsize=(15, 12))
    
    # 1. æ ‡ç­¾åˆ†å¸ƒé¥¼å›¾
    plt.subplot(2, 2, 1)
    unique_labels, counts = np.unique(labels, return_counts=True)
    plt.pie(counts, labels=[f'ç±»åˆ« {l}' for l in unique_labels], autopct='%1.1f%%')
    plt.title('WSIæ ‡ç­¾åˆ†å¸ƒ')
    
    # 2. ç‰¹å¾å€¼åˆ†å¸ƒ
    plt.subplot(2, 2, 2)
    # éšæœºé€‰æ‹©ä¸€äº›ç‰¹å¾ç»´åº¦
    sample_dims = np.random.choice(features.shape[1], 5, replace=False)
    for dim in sample_dims:
        plt.hist(features[:, dim], bins=30, alpha=0.6, label=f'ç»´åº¦{dim}')
    plt.xlabel('ç‰¹å¾å€¼')
    plt.ylabel('é¢‘æ¬¡')
    plt.title('ç‰¹å¾å€¼åˆ†å¸ƒ (éšæœº5ä¸ªç»´åº¦)')
    plt.legend()
    
    # 3. PCAå¯è§†åŒ–
    plt.subplot(2, 2, 3)
    from sklearn.decomposition import PCA
    pca = PCA(n_components=2)
    features_2d = pca.fit_transform(features)
    
    # å…³é”®ä¿®å¤ï¼šå°†å­—ç¬¦ä¸²æ ‡ç­¾è½¬æ¢ä¸ºæ•°å€¼ç±»å‹
    # åˆ›å»ºæ ‡ç­¾æ˜ å°„å­—å…¸
    label_to_num = {label: idx for idx, label in enumerate(unique_labels)}
    # å°†å­—ç¬¦ä¸²æ ‡ç­¾è½¬æ¢ä¸ºæ•°å­—
    numeric_labels = np.array([label_to_num[label] for label in labels])
    
    scatter = plt.scatter(features_2d[:, 0], features_2d[:, 1], 
                        c=numeric_labels, cmap='viridis', alpha=0.7)
    plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%})')
    plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%})')
    plt.title('ç‰¹å¾ç©ºé—´åˆ†å¸ƒ (PCA)')
    
    # åˆ›å»ºè‡ªå®šä¹‰é¢œè‰²æ¡ï¼Œæ˜¾ç¤ºåŸå§‹æ ‡ç­¾åç§°
    cbar = plt.colorbar(scatter, ticks=range(len(unique_labels)))
    cbar.set_ticklabels(unique_labels)
    
    # 4. ç±»åˆ«é—´ç‰¹å¾å·®å¼‚
    plt.subplot(2, 2, 4)
    from sklearn.metrics import pairwise_distances
    intra_dists = []
    inter_dists = []
    
    for label in unique_labels:
        class_features = features[labels == label]
        other_features = features[labels != label]
        
        if len(class_features) > 1:
            intra_dist = pairwise_distances(class_features).mean()
            intra_dists.append(intra_dist)
        
        if len(other_features) > 0:
            inter_dist = pairwise_distances(class_features, other_features).mean()
            inter_dists.append(inter_dist)
    
    separation_ratio = np.mean(inter_dists) / np.mean(intra_dists)
    
    plt.bar(['ç±»å†…è·ç¦»', 'ç±»é—´è·ç¦»'], [np.mean(intra_dists), np.mean(inter_dists)])
    plt.ylabel('å¹³å‡è·ç¦»')
    plt.title(f'ç‰¹å¾åˆ†ç¦»åº¦: {separation_ratio:.3f}')
    
    # åœ¨å›¾ä¸Šæ·»åŠ åˆ†ç¦»åº¦è¯„ä¼°
    if separation_ratio > 1.5:
        evaluation = "ä¼˜ç§€"
    elif separation_ratio > 1.2:
        evaluation = "è‰¯å¥½"
    elif separation_ratio > 1.0:
        evaluation = "ä¸€èˆ¬"
    else:
        evaluation = "è¾ƒå·®"
    
    plt.text(0.5, 0.9, f'è¯„ä¼°: {evaluation}', 
             transform=plt.gca().transAxes, ha='center', 
             bbox=dict(boxstyle="round,pad=0.3", facecolor="lightblue"))
    
    plt.tight_layout()
    # ä¿å­˜å›¾è¡¨åˆ°æ–‡ä»¶
    plt.savefig('aggregated_features_analysis.png', dpi=300, bbox_inches='tight')
    
    # ç¡®ä¿å›¾è¡¨æ˜¾ç¤º
    print("å›¾è¡¨å·²ç”Ÿæˆï¼ŒæŒ‰ä»»æ„é”®å…³é—­å›¾è¡¨ç»§ç»­...")
    plt.show(block=True)  # è¿™ä¼šé˜»å¡ç¨‹åºç›´åˆ°å…³é—­å›¾è¡¨çª—å£
    
    print(f"ğŸ¯ ç‰¹å¾åˆ†ç¦»åº¦: {separation_ratio:.3f} ({evaluation})")
    return separation_ratio

def analyze_class_separation(features, labels):
    """åˆ†æç±»åˆ«é—´çš„åˆ†ç¦»ç¨‹åº¦"""
    
    print("\nğŸ”¬ æ·±åº¦åˆ†æç±»åˆ«åˆ†ç¦»...")
    
    from sklearn.metrics import pairwise_distances
    unique_labels = np.unique(labels)
    
    # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„ä¸­å¿ƒ
    class_centers = {}
    for label in unique_labels:
        class_centers[label] = np.mean(features[labels == label], axis=0)
    
    # è®¡ç®—ç±»åˆ«ä¸­å¿ƒä¹‹é—´çš„è·ç¦»
    print("ç±»åˆ«ä¸­å¿ƒä¹‹é—´çš„è·ç¦»:")
    for i, label1 in enumerate(unique_labels):
        for label2 in unique_labels[i+1:]:
            dist = np.linalg.norm(class_centers[label1] - class_centers[label2])
            print(f"  ç±»åˆ« {label1} â†” ç±»åˆ« {label2}: {dist:.3f}")
    
    # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„ç´§å¯†åº¦
    print("\nå„ç±»åˆ«ç´§å¯†åº¦ (ç±»å†…å¹³å‡è·ç¦»):")
    for label in unique_labels:
        class_features = features[labels == label]
        if len(class_features) > 1:
            intra_dist = pairwise_distances(class_features).mean()
            print(f"  ç±»åˆ« {label}: {intra_dist:.3f}")
    
    return class_centers

# ä¸»æ‰§è¡Œå‡½æ•°
def main():
    """ä¸»æ‰§è¡Œå‡½æ•°"""
    
    # 1. æ•°æ®æ£€æŸ¥
    features, labels = correct_data_analysis('features.npy', 'labels.npy')
    
    if features is not None:
        # 2. å¯è§†åŒ–åˆ†æ
        separation_ratio = visualize_aggregated_features(features, labels)
        
        # 3. æ·±åº¦åˆ†æ
        class_centers = analyze_class_separation(features, labels)
        
        # 4. ä¿å­˜åˆ†æç»“æœ
        analysis_report = {
            'data_shape': features.shape,
            'label_distribution': dict(zip(*np.unique(labels, return_counts=True))),
            'feature_stats': {
                'min': float(features.min()),
                'max': float(features.max()),
                'mean': float(features.mean()),
                'std': float(features.std())
            },
            'separation_ratio': float(separation_ratio)
        }
        
        print("\nâœ… æ•°æ®åˆ†æå®Œæˆ!")
        print("ğŸ“‹ ä¸‹ä¸€æ­¥å»ºè®®:")
        
        if separation_ratio > 1.5:
            print("   ğŸ‰ ç‰¹å¾è´¨é‡ä¼˜ç§€ï¼Œå¯ä»¥ç›´æ¥å¼€å§‹åˆ†ç±»å™¨ä¼˜åŒ–")
        elif separation_ratio > 1.2:
            print("   ğŸ‘ ç‰¹å¾è´¨é‡è‰¯å¥½ï¼Œå»ºè®®å…ˆåšç‰¹å¾é€‰æ‹©å†ä¼˜åŒ–åˆ†ç±»å™¨")
        else:
            print("   âš ï¸  ç‰¹å¾åˆ†ç¦»åº¦ä¸€èˆ¬ï¼Œå»ºè®®æ¢ç´¢ç‰¹å¾å¢å¼ºæ–¹æ³•")
        
        return features, labels, analysis_report
    
    return None, None, None

if __name__ == "__main__":
    features, labels, report = main()

'''
ğŸ“Š ç‰¹å¾å½¢çŠ¶: (400, 1024)
ğŸ“Š ç‰¹å¾å½¢çŠ¶: (400, 1024)
ğŸ·ï¸  æ ‡ç­¾å½¢çŠ¶: (400,)
âœ… æ•°æ®æ ¼å¼: å·²èšåˆçš„ç‰¹å¾
ğŸ“ˆ WSIæ•°é‡: 400
ğŸ¯ ç‰¹å¾ç»´åº¦: 1024
ğŸ“‹ æ ‡ç­¾åˆ†å¸ƒ:
ğŸ¯ ç‰¹å¾ç»´åº¦: 1024
ğŸ“‹ æ ‡ç­¾åˆ†å¸ƒ:
  ç±»åˆ« Benign: 100 ä¸ªæ ·æœ¬ (25.0%)
  ç±»åˆ« InSitu: 100 ä¸ªæ ·æœ¬ (25.0%)
  ç±»åˆ« Invasive: 100 ä¸ªæ ·æœ¬ (25.0%)
  ç±»åˆ« Normal: 100 ä¸ªæ ·æœ¬ (25.0%)
ğŸ” ç‰¹å¾å€¼èŒƒå›´: [-11.308, 15.353]
ğŸ“ ç‰¹å¾å‡å€¼: 0.008 Â± 1.514
ğŸ§¹ æ•°æ®æ¸…æ´åº¦ - NaN: 0, æ— é™å€¼: 0
 ç‰¹å¾åˆ†ç¦»åº¦: 1.182 (ä¸€èˆ¬)

ğŸ”¬ æ·±åº¦åˆ†æç±»åˆ«åˆ†ç¦»...
ç±»åˆ«ä¸­å¿ƒä¹‹é—´çš„è·ç¦»:
  ç±»åˆ« Benign â†” ç±»åˆ« InSitu: 6.717
  ç±»åˆ« Benign â†” ç±»åˆ« Invasive: 12.881
  ç±»åˆ« Benign â†” ç±»åˆ« Normal: 7.575
  ç±»åˆ« InSitu â†” ç±»åˆ« Invasive: 17.038
  ç±»åˆ« InSitu â†” ç±»åˆ« Normal: 3.741
  ç±»åˆ« Invasive â†” ç±»åˆ« Normal: 19.104

å„ç±»åˆ«ç´§å¯†åº¦ (ç±»å†…å¹³å‡è·ç¦»):
  ç±»åˆ« Benign: 17.772
  ç±»åˆ« InSitu: 18.039
  ç±»åˆ« Invasive: 21.426
  ç±»åˆ« Normal: 17.911
'''